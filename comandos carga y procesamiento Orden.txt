==================================================================================================================

FASE 1: CONFIGURACIÓN DEL ENTORNO HADOOP

docker build -t hadoop-custom:latest .
docker-compose up -d
docker cp "C:\Users\SumonBP\Desktop\Uce\8voSem\taller1\taller1\palabras\generar_1gb.py" namenode:/tmp/
apt-get update && apt-get install -y python3
ls -la /tmp/generar_1gb.py

==================================================================================================================

FASE 2: GENERACIÓN Y CARGA DEL DATASET

python3 /tmp/generar_1gb.py
ls -lh /tmp/archivo_1gb.txt
wc -l /tmp/archivo_1gb.txt
hdfs dfs -mkdir /ejercicio_mapreduce
hdfs dfs -ls
hdfs dfs -mkdir -p /ejercicio_mapreduce/input
hdfs dfs -put /tmp/archivo_1gb.txt /ejercicio_mapreduce/input
hdfs dfs -ls -h /ejercicio_mapreduce/input/

==================================================================================================================

FASE 3: PROCESAMIENTO MAPREDUCE DISTRIBUIDO

docker exec -it namenode bash
mkdir -p /tmp/split_work
cd /tmp/split_work
hdfs dfs -cat /ejercicio_mapreduce/input/* | split -l 1000000 - part_
cd /tmp/split_work/
hdfs dfs -mkdir -p /ejercicio_mapreduce/input_small

for file in part_aa part_ab part_ac part_ad part_ae part_af part_ag part_ah part_ai part_aj part_ak part_al part_am part_an part_ao part_ap part_aq part_ar part_as part_at; do
    if [ -f "$file" ]; then
        echo "=== Procesando: $file ==="
        hdfs dfs -rm -r /ejercicio_mapreduce/input_small/*
        hdfs dfs -put $file /ejercicio_mapreduce/input_small/
        hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.2.jar \
        wordcount /ejercicio_mapreduce/input_small /ejercicio_mapreduce/output_$file
        echo "=== Completado: $file ==="
    fi
done

hdfs dfs -du -h /ejercicio_mapreduce/output_*
hdfs dfs -cat /ejercicio_mapreduce/output_part_aa/part-r-00000 | head -5
hdfs dfs -cat /ejercicio_mapreduce/output_part_ab/part-r-00000 | head -5

==================================================================================================================

FASE 4: COMBINACIÓN DE RESULTADOS

mkdir -p /tmp/final_merge
cd /tmp/final_merge

for part in aa ab ac ad ae af ag ah ai aj ak al am an ao ap aq ar as at; do
    hdfs dfs -get /ejercicio_mapreduce/output_part_$part/part-r-00000 part_$part.txt
    echo "Descargado: part_$part.txt"
done

echo "Combinando resultados..."
cat part_*.txt | awk '{count[$1] += $2} END {for (word in count) print word, count[word]}' | sort > final_result.txt

echo "=== Primeras 10 palabras ==="
head -10 final_result.txt
echo "=== Total de palabras únicas: $(wc -l final_result.txt) ==="

hdfs dfs -rm -r /ejercicio_mapreduce/output_final
hdfs dfs -mkdir -p /ejercicio_mapreduce/output_final
hdfs dfs -put final_result.txt /ejercicio_mapreduce/output_final/

echo "¡Proceso completado exitosamente!"